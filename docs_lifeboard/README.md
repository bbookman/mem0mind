# Lifeboard MVP - Documentation

Welcome to the Lifeboard MVP documentation. Lifeboard is an interactive reflection space and powerful planning assistant. This MVP focuses on core data ingestion, a calendar-centric interface, and AI-powered daily summaries and chat.

## Overview

This project aims to provide a self-hostable, local-first application for personal data aggregation and AI-assisted reflection. It integrates data from sources like Limitless and Bee.computer, alongside weather and mood tracking, and uses local AI models via Ollama for summarization and chat.

## Key Features (MVP)

*   **Automated Data Ingestion**: Pulls data from configured sources.
    *   Limitless lifelogs
    *   Bee.computer activities
    *   Weather data
    *   Mood entries (via webhook)
*   **Calendar Interface**:
    *   Monthly view highlighting days with data.
    *   Daily "newspaper" view presenting aggregated data and AI summaries.
*   **AI-Powered Summaries**: Daily reflective summaries generated by a local LLM.
*   **AI Chat Interface**: Allows users to "chat with their life" using a local LLM, with context from their data.
*   **Local-First Architecture**: All core components (database, vector store, LLM, application logic) are designed to run locally via Docker.

## Technology Stack

*   **Backend Orchestration**: n8n (custom Docker image with Python environment)
*   **AI Memory & Vector Store**: Mem0 library with Qdrant as the vector database.
*   **Local LLM/Embeddings**: Ollama (e.g., using Llama3.1, nomic-embed-text).
*   **Database (n8n & App Data)**: PostgreSQL.
*   **Frontend**: React (served by Nginx in Docker).
*   **Containerization**: Docker and Docker Compose.

## Getting Started (Local Deployment)

This MVP is designed for technical users comfortable with Docker and environment configuration.

1.  **Prerequisites**:
    *   Docker Desktop or Docker Engine with Docker Compose installed.
    *   Git (to clone the repository).
    *   A terminal or command prompt.

2.  **Clone the Repository**:
    ```bash
    git clone <repository_url>
    cd lifeboard-mvp # Or your repository directory name
    ```

3.  **Configuration**:
    *   Copy the example environment file:
        ```bash
        cp config_lifeboard/config.example.env .env
        ```
        (Or `cp config_lifeboard/config.example.env config_lifeboard/.env` if your `docker-compose.yml` uses `env_file: ./config_lifeboard/.env`)
    *   Review and **edit the `.env` file**. At a minimum, you **must** set a secure `N8N_ENCRYPTION_KEY`.
    *   Update other variables as needed (e.g., PostgreSQL credentials if you don't want defaults, Ollama models if you prefer others, API keys if you intend to use live data sources instead of relying on n8n's credential manager for them). Refer to comments in `config.example.env`.

4.  **Build and Run Services**:
    ```bash
    docker-compose up --build -d
    ```
    *   `--build`: Builds the custom n8n and frontend images.
    *   `-d`: Runs services in detached mode.
    *   This will start all services: PostgreSQL, Qdrant, Ollama, n8n, and the frontend.
    *   The first run might take some time to download Docker images and build application images.

5.  **Pull Ollama Models (One-time setup)**:
    *   After the services are up, you need to pull the LLM and embedding models for Ollama if they are not already available in your `ollama_data` volume.
    *   Open a new terminal and run:
        ```bash
        docker-compose exec ollama ollama pull ${OLLAMA_LLM_MODEL:-llama3.1}
        docker-compose exec ollama ollama pull ${OLLAMA_EMBEDDER_MODEL:-nomic-embed-text}
        ```
        (Replace model names if you configured different ones in your `.env` file).
    *   You can check available models with `docker-compose exec ollama ollama list`.

6.  **Accessing Services**:
    *   **Lifeboard Frontend**: `http://localhost:3000` (or as configured in `docker-compose.yml`)
    *   **n8n UI**: `http://localhost:5678` (or as configured)
        *   Set up your n8n admin user on first access.
        *   The Lifeboard n8n workflows should be available due to the volume mount. You may need to activate them.
    *   **Qdrant UI (optional)**: `http://localhost:6333/dashboard` (if Qdrant exposes a dashboard on its main port)
    *   **Ollama API (optional)**: `http://localhost:11434` (check with `/api/tags`)

7.  **Using Lifeboard**:
    *   **Data Ingestion**:
        *   Limitless & Bee.computer: These workflows are set to run on a schedule (daily). For initial testing, you can trigger them manually from the n8n UI. You'll need to configure API credentials within n8n's credential manager for these to fetch real data.
        *   Weather: Runs daily. Ensure `USER_LATITUDE` and `USER_LONGITUDE` are set in `.env`.
        *   Mood: Submit mood data via its webhook. The URL can be found in the "Mood Webhook Trigger" node in the `ingestion_mood` workflow in n8n (e.g., `http://localhost:5678/webhook/your-mood-webhook-id`).
    *   **Frontend**:
        *   Use the calendar to navigate days.
        *   View daily summaries and module data.
        *   Interact with the AI chat widget.

8.  **Stopping Services**:
    ```bash
    docker-compose down
    ```
    *   To remove persistent data volumes as well (for a clean start): `docker-compose down -v`

## Further Information

*   **Developer Guide**: See `developer_guide.md` for more details on development, configuration of specific data sources, and n8n workflow structure.
*   **Mem0 Setup**: See `mem0_setup.md` for details on how Mem0, Qdrant, and Ollama are configured.
*   **Manual Verification**: See `tests_lifeboard/deployment/MANUAL_VERIFICATION.md` for steps to manually test the deployed application.

This MVP provides a foundation. Future development could include more data sources, enhanced AI capabilities, a more polished UI, and easier installation for non-technical users.
